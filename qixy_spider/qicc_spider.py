# -*- coding: utf-8 -*-
# @Time    : 2019/10/18 11:32
# @Author  : zgh
# @Email   : 849080458@qq.com
# @File    : qicc_spider.py
# @Software: PyCharm


from bs4 import BeautifulSoup
import requests
import xlrd
import xlwt
from xlutils.copy import copy
import time
import winsound


# 企查查网站爬虫类
class EnterpriseInfoSpider:
    def __init__(self):

        # 文件相关
        self.excelPath = 'enterprise_data.xls'
        self.sheetName = 'details'
        self.workbook = None
        self.table = None
        self.beginRow = None

        # 目录页
        self.catalogUrl = "http://www.qichacha.com/search_index"

        # 详情页（前缀+firmXXXX+后缀）
        self.detailsUrl = "http://www.qichacha.com/company_getinfos"

        self.cookie = input("请输入cookie:").encode("utf-8").decode("gbk")
        self.host = "www.qichacha.com"
        self.userAgent = "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36"

        self.headers = {
            "cookie": self.cookie,
            "host": self.host,
            "user-agent": self.userAgent
        }

        # 数据字段名17个
        self.fields = ['公司名称', '电话号码', '邮箱', '统一社会信用代码', '注册号', '组织机构代码', '经营状态', '公司类型', '成立日期', '法定代表人', '注册资本',
                       '营业期限', '登记机关', '发照日期', '公司规模', '所属行业', '英文名', '曾用名', '企业地址', '经营范围']

    # 爬虫开始前的一些预处理
    def init(self):

        try:
            # 试探是否有该excel文件，#获取行数：workbook.sheets()[0].nrows
            readWorkbook = xlrd.open_workbook(self.excelPath)
            self.beginRow = readWorkbook.sheets()[0].nrows  # 获取行数
            self.workbook = copy(readWorkbook)
            self.table = self.workbook.get_sheet(0)

        except Exception :
            self.workbook = xlwt.Workbook(encoding='utf-8')
            self.table = self.workbook.add_sheet(self.sheetName)

            # 创建表头字段
            col = 0
            for field in self.fields:
                self.table.write(0, col, field.encode('gbk').decode('gbk'))
                col += 1

            self.workbook.save(self.excelPath)
            self.beginRow = 1
            print("已在当前目录下创建enterprise_data.xls数据表")

    # 从keyword/1页 得到的html中获得总页码数
    def getTotalPage(self, catalogPageCode):
        soup = BeautifulSoup(catalogPageCode, "html.parser")
        pagebar = soup.select("li #ajaxpage")
        if pagebar == None or pagebar == []:
            return -1
        return int(soup.select("li #ajaxpage")[-1].string.strip(' .'))

    # 从keyword/page页 得到html中的所有公司条目
    def getFirmIdDoms(self, catalogPageCode):
        soup = BeautifulSoup(catalogPageCode, "html.parser")
        return soup.select("#searchlist .table-search-list .tp2 a")

    # 爬虫开始
    def start(self):
        keyword = input("请输入关键字：").encode("gbk").decode("gbk")
        while keyword != "end":
            # 先获取keyword第一页内容的页码
            totalPage = self.getTotalPage(self.getCatalogPageCode(keyword, 1))
            if totalPage == -1:
                # 请求下一轮查询的关键字
                keyword = input("爬取结束,请输入关键字：").encode("gbk").decode("gbk")
                continue

            # 模拟翻页操作
            for page in range(1, totalPage + 1):

                print("正在爬取第", page, "页的数据,请稍等...")

                # 获取第page页代码
                catalogPageCode = self.getCatalogPageCode(keyword, page)
                firmIdDoms = self.getFirmIdDoms(catalogPageCode)
                for firmIdDom in firmIdDoms:
                    firmId = firmIdDom['href'][6:-6]
                    companyname = ""  # 公司名
                    for string in firmIdDom.strings:
                        companyname += string

                    tdDom = firmIdDom.find_parent().find_parent()
                    phoneDom = tdDom.select('.i-phone3')
                    emailDom = tdDom.select('.fa-envelope-o')
                    phone = ""
                    email = ""
                    if phoneDom != None and phoneDom != []:
                        phone = phoneDom[0].next_sibling.strip()  # 手机
                    if emailDom != None and emailDom != []:
                        email = emailDom[0].next_sibling.strip()  # 邮箱

                    detailsPageCode = self.getDetailsPageCode(firmId, companyname)
                    self.writeDetailsToExcel(detailsPageCode, companyname, phone, email)
                    time.sleep(0.3)  # 0.5s后再爬防止反爬虫机制

            # 请求下一轮查询的关键字
            keyword = input("爬取结束,请输入关键字：").encode("utf-8").decode("gbk")

        print("爬虫已完全结束！")

    # 根据keyword和page构造查询串
    # 其中keyword中的空格换成+
    # 返回查询字符串构成的字典
    def getCatalogQueryString(self, keyword, page):
        keyword.replace(' ', '+')
        return {"key": keyword, "index": "0", "p": page}

    def getDetailQueryString(self, firmId, companyname):
        return {"unique": firmId, "companyname": companyname, "tab": "base"}

    # 根据keyword关键字获取目录页代码
    def getCatalogPageCode(self, keyword, page):
        queryString = self.getCatalogQueryString(keyword, page)
        response = requests.request("GET", self.catalogUrl, headers=self.headers, params=queryString)
        return response.text

    # 根据firmId获取公司的详情页代码
    def getDetailsPageCode(self, firmId, companyname):
        queryString = self.getDetailQueryString(firmId, companyname)
        response = requests.request("GET", self.detailsUrl, headers=self.headers, params=queryString)
        return response.text

    # 抓取detailsPageCode页上该企业所有信息，并存入excel
    def writeDetailsToExcel(self, detailsPageCode, companyname, phone, email):
        detailDoms = self.getDetailDoms(detailsPageCode)

        self.table.write(self.beginRow, 0, companyname)
        self.table.write(self.beginRow, 1, phone)
        self.table.write(self.beginRow, 2, email)

        col = 3
        for detailDom in detailDoms:
            detailName = detailDom.label.string.strip()[:-1]
            detailValue = detailDom.label.next_sibling.string.strip()
            while col < len(self.fields):
                # 找到匹配的那列字段
                if detailName == self.fields[col].decode('gbk'):
                    self.table.write(self.beginRow, col, detailValue)  # 写入excel
                    col += 1
                    break
                else:
                    col += 1
        self.workbook.save(self.excelPath)  # 保存至文件
        self.beginRow += 1

    # 根据detailsPageCode获得它的所有detailDoms元素
    def getDetailDoms(self, detailsPageCode):
        soup = BeautifulSoup(detailsPageCode, "html.parser")
        return soup.select(".company-base li")


########
# 爬虫入口
#QCCSESSID=qiu3li5od6vqbc7ticagcdkf55; zg_did=%7B%22did%22%3A%20%2216ddf6e758610-004156de1bf394-b363e65-1fa400-16ddf6e7587f%22%7D; UM_distinctid=16ddf6e76551f2-05cf771d78f1ad-b363e65-1fa400-16ddf6e76561c7; CNZZDATA1254842228=1156582865-1571409785-%7C1571409785; hasShow=1; _uab_collina=157141162179223579793437; acw_tc=701dc8d015714116448377003ea1ff161ac2a19c178e879cff99b9bc6a; Hm_lvt_3456bee468c83cc63fb5147f119f1075=1571411622; Hm_lpvt_3456bee468c83cc63fb5147f119f1075=1571411963; zg_de1d1a35bfa24ce29bbf2c7eb17e6c4f=%7B%22sid%22%3A%201571411621261%2C%22updated%22%3A%201571411963122%2C%22info%22%3A%201571411621268%2C%22superProperty%22%3A%20%22%7B%7D%22%2C%22platform%22%3A%20%22%7B%7D%22%2C%22utm%22%3A%20%22%7B%7D%22%2C%22referrerDomain%22%3A%20%22%22%7D

########
spider = EnterpriseInfoSpider()
spider.init()
spider.start()
